{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f5577a-f65f-4236-952c-28514d017158",
   "metadata": {},
   "source": [
    "**Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee390b-83e5-4d65-aab6-a9aa0ea14d77",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "R-squared (R²), also known as the coefficient of determination, is a statistical measure in linear regression models that represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "### Concept\n",
    "- **R-squared** quantifies the goodness of fit of a regression model. It indicates how well the data points fit the model, or more precisely, the proportion of the variance in the dependent variable that can be explained by the independent variables.\n",
    "- **Range**: R-squared values range from 0 to 1.\n",
    "  - **0**: Indicates that the model explains none of the variance in the dependent variable.\n",
    "  - **1**: Indicates that the model explains all the variance in the dependent variable.\n",
    "\n",
    "### Calculation\n",
    "R-squared is calculated using the following formula:\n",
    "\n",
    "\\[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\n",
    "\n",
    "Where:\n",
    "- \\( SS_{res} \\) is the **sum of squares of the residuals** (also called the residual sum of squares, SSR). This measures the discrepancy between the observed data and the values predicted by the model.\n",
    "- \\( SS_{tot} \\) is the **total sum of squares**. This measures the total variance in the dependent variable, and is calculated as the sum of the squared differences between each observed value and the mean of the dependent variable.\n",
    "\n",
    "### Representation\n",
    "- **Interpretation**: R-squared represents the proportion of the total variance in the dependent variable that is explained by the independent variables in the model.\n",
    "  - **High R-squared**: Indicates a better fit of the model to the data, meaning that the independent variables explain a large portion of the variance in the dependent variable.\n",
    "  - **Low R-squared**: Indicates a poor fit, meaning that the independent variables do not explain much of the variance in the dependent variable.\n",
    "\n",
    "### Example\n",
    "Suppose you have a dataset with a dependent variable \\( Y \\) and an independent variable \\( X \\). You fit a linear regression model to this data and obtain the following values:\n",
    "- \\( Y \\) (observed values)\n",
    "- \\( \\hat{Y} \\) (predicted values from the model)\n",
    "- \\( \\bar{Y} \\) (mean of the observed values)\n",
    "\n",
    "The residual sum of squares \\( SS_{res} \\) and the total sum of squares \\( SS_{tot} \\) are calculated as follows:\n",
    "\n",
    "\\[ SS_{res} = \\sum (Y_i - \\hat{Y}_i)^2 \\]\n",
    "\\[ SS_{tot} = \\sum (Y_i - \\bar{Y})^2 \\]\n",
    "\n",
    "Then, the R-squared value is:\n",
    "\n",
    "\\[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\n",
    "\n",
    "In summary, R-squared provides a measure of how well the independent variables in a regression model explain the variability of the dependent variable, helping to assess the model's performance and goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949558a-7168-4e5e-ac7c-5e67312aff65",
   "metadata": {},
   "source": [
    "**Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36720a3a-269c-4e65-b546-63ce6cf82535",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "Adjusted R-squared is a modified version of the R-squared (coefficient of determination) that adjusts for the number of predictors in a regression model. It provides a more accurate measure of the goodness of fit, especially when multiple independent variables are included in the model.\n",
    "\n",
    "### Definition\n",
    "Adjusted R-squared takes into account the number of independent variables (predictors) and the sample size, providing a penalty for adding irrelevant predictors to the model. This adjustment helps prevent overfitting, where the model becomes too complex and fits the noise in the data rather than the true underlying relationship.\n",
    "\n",
    "### Formula\n",
    "The formula for adjusted R-squared is:\n",
    "\n",
    "\\[ \\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right) \\]\n",
    "\n",
    "Where:\n",
    "- \\( R^2 \\) is the regular R-squared.\n",
    "- \\( n \\) is the number of observations (sample size).\n",
    "- \\( k \\) is the number of independent variables (predictors).\n",
    "\n",
    "### Differences from Regular R-squared\n",
    "1. **Adjustment for Predictors**:\n",
    "   - **Regular R-squared**: Increases (or at least does not decrease) with the addition of more predictors, regardless of their relevance.\n",
    "   - **Adjusted R-squared**: Adjusts for the number of predictors and only increases if the added predictors improve the model more than would be expected by chance. It can decrease if the new predictors do not add significant explanatory power.\n",
    "\n",
    "2. **Bias Towards Overfitting**:\n",
    "   - **Regular R-squared**: Can be misleading in models with many predictors, as it may suggest a better fit even when the added predictors do not truly improve the model.\n",
    "   - **Adjusted R-squared**: Provides a more accurate measure of the goodness of fit by penalizing the inclusion of unnecessary predictors, thus reducing the risk of overfitting.\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - **Regular R-squared**: Represents the proportion of the variance in the dependent variable that is explained by the independent variables, without considering the number of predictors.\n",
    "   - **Adjusted R-squared**: Represents the proportion of the variance in the dependent variable that is explained by the independent variables, while adjusting for the number of predictors, providing a more honest assessment of the model's explanatory power.\n",
    "\n",
    "### Example\n",
    "Consider a dataset where we fit two regression models:\n",
    "- **Model 1**: A simple linear regression with one predictor.\n",
    "- **Model 2**: A multiple linear regression with several predictors.\n",
    "\n",
    "For both models, we can calculate the regular R-squared. However, the adjusted R-squared will differ:\n",
    "- **Model 1**: The adjusted R-squared will be close to the regular R-squared since there's only one predictor.\n",
    "- **Model 2**: The adjusted R-squared will be lower than the regular R-squared if the added predictors do not significantly improve the model's explanatory power.\n",
    "\n",
    "In summary, adjusted R-squared provides a more accurate and reliable measure of the goodness of fit for regression models with multiple predictors by adjusting for the number of predictors and the sample size, helping to avoid overfitting and providing a clearer picture of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06c06c-6b5f-4878-b170-c055c1cb1fda",
   "metadata": {},
   "source": [
    "**Q3. When is it more appropriate to use adjusted R-squared?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b60770-fec4-4e8f-a4e1-7779798d08d2",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "Adjusted R-squared is more appropriate to use in the following situations:\n",
    "\n",
    "### 1. Multiple Linear Regression Models\n",
    "- When you have multiple independent variables in your regression model, adjusted R-squared is more appropriate because it accounts for the number of predictors. It helps to avoid the misleading increase in R-squared that occurs simply by adding more variables, even if they do not improve the model.\n",
    "\n",
    "### 2. Model Comparison\n",
    "- When comparing different regression models with different numbers of predictors, adjusted R-squared provides a better basis for comparison. It penalizes models with unnecessary predictors, ensuring that you choose a model that balances complexity and explanatory power.\n",
    "\n",
    "### 3. Preventing Overfitting\n",
    "- Adjusted R-squared helps prevent overfitting by penalizing the addition of irrelevant predictors. Overfitting occurs when a model becomes too complex and captures the noise in the data rather than the true underlying pattern. Adjusted R-squared ensures that only predictors that provide a real improvement in the model’s performance are rewarded.\n",
    "\n",
    "### 4. Assessing Model Improvement\n",
    "- When adding new predictors to an existing model, adjusted R-squared allows you to assess whether the new predictors genuinely improve the model. An increase in adjusted R-squared indicates that the new predictors contribute valuable information, while a decrease suggests that they may not be necessary.\n",
    "\n",
    "### 5. Evaluating Model Performance with Small Sample Sizes\n",
    "- In situations with small sample sizes, regular R-squared can be overly optimistic about the model's fit. Adjusted R-squared provides a more conservative and realistic measure of model performance by considering the degrees of freedom.\n",
    "\n",
    "### Example Scenarios\n",
    "- **Example 1**: You are building a regression model to predict house prices based on several features such as size, number of bedrooms, location, etc. As you add more features, you want to ensure that each new feature genuinely improves the model. Adjusted R-squared helps you determine this.\n",
    "- **Example 2**: You have two models: one with three predictors and another with ten predictors. Regular R-squared may show a higher value for the more complex model, but adjusted R-squared will indicate if the additional predictors are not providing meaningful improvements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5ebbb-184e-4b14-a591-2048a8870f3e",
   "metadata": {},
   "source": [
    "**Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9f424-bc14-4d59-b08a-bf7cad208167",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "In regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are metrics used to evaluate the performance of a regression model by quantifying the difference between observed and predicted values.\n",
    "\n",
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "#### Definition\n",
    "MSE is the average of the squared differences between the observed actual outcomes and the outcomes predicted by the model.\n",
    "\n",
    "#### Formula\n",
    "\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 \\]\n",
    "\n",
    "Where:\n",
    "- \\( Y_i \\) is the observed value.\n",
    "- \\( \\hat{Y}_i \\) is the predicted value.\n",
    "- \\( n \\) is the number of observations.\n",
    "\n",
    "#### Representation\n",
    "- **Unit**: The unit of MSE is the square of the unit of the dependent variable.\n",
    "- **Interpretation**: MSE gives more weight to larger errors due to the squaring process, which can be useful if large errors are particularly undesirable.\n",
    "\n",
    "### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "#### Definition\n",
    "RMSE is the square root of the MSE. It brings the unit of the error back to the same unit as the dependent variable, making it more interpretable.\n",
    "\n",
    "#### Formula\n",
    "\\[ \\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2} \\]\n",
    "\n",
    "#### Representation\n",
    "- **Unit**: The same as the unit of the dependent variable.\n",
    "- **Interpretation**: RMSE provides a measure of the average magnitude of the errors in the same unit as the dependent variable, making it easier to interpret and compare.\n",
    "\n",
    "### 3. Mean Absolute Error (MAE)\n",
    "\n",
    "#### Definition\n",
    "MAE is the average of the absolute differences between the observed actual outcomes and the outcomes predicted by the model.\n",
    "\n",
    "#### Formula\n",
    "\\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |Y_i - \\hat{Y}_i| \\]\n",
    "\n",
    "Where:\n",
    "- \\( Y_i \\) is the observed value.\n",
    "- \\( \\hat{Y}_i \\) is the predicted value.\n",
    "- \\( n \\) is the number of observations.\n",
    "\n",
    "#### Representation\n",
    "- **Unit**: The same as the unit of the dependent variable.\n",
    "- **Interpretation**: MAE provides a straightforward measure of the average magnitude of the errors without considering their direction. It is less sensitive to large errors compared to MSE and RMSE.\n",
    "\n",
    "### Summary of Differences\n",
    "- **MSE**: Penalizes larger errors more due to squaring, making it sensitive to outliers.\n",
    "- **RMSE**: Similar to MSE but in the same unit as the dependent variable, providing an interpretable measure of error.\n",
    "- **MAE**: Measures the average magnitude of errors in the same unit as the dependent variable, less sensitive to outliers compared to MSE and RMSE.\n",
    "\n",
    "### Example\n",
    "Consider a dataset with actual values \\( Y \\) and predicted values \\( \\hat{Y} \\):\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\text{Observed values (Y)} &: [2, 4, 6, 8] \\\\\n",
    "\\text{Predicted values (}\\hat{Y}\\text{)} &: [3, 4, 4, 10] \\\\\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "- **Errors**: \\( [1, 0, -2, 2] \\)\n",
    "- **Squared errors**: \\( [1, 0, 4, 4] \\)\n",
    "- **Absolute errors**: \\( [1, 0, 2, 2] \\)\n",
    "\n",
    "Calculations:\n",
    "- **MSE**: \\( \\frac{1+0+4+4}{4} = 2.25 \\)\n",
    "- **RMSE**: \\( \\sqrt{2.25} \\approx 1.5 \\)\n",
    "- **MAE**: \\( \\frac{1+0+2+2}{4} = 1.25 \\)\n",
    "\n",
    "In summary, RMSE, MSE, and MAE are essential metrics in regression analysis for evaluating model performance, each offering different insights into the nature of prediction errors and their impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816f495-4ff0-4398-a633-463d69926465",
   "metadata": {},
   "source": [
    "**Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5290a9a-7061-4c8c-9623-b8826c486668",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "### Advantages and Disadvantages of RMSE, MSE, and MAE in Regression Analysis\n",
    "\n",
    "#### 1. Mean Squared Error (MSE)\n",
    "\n",
    "##### Advantages:\n",
    "1. **Sensitivity to Large Errors**: MSE penalizes larger errors more due to the squaring, which can be beneficial if large errors are particularly undesirable.\n",
    "2. **Differentiability**: MSE is differentiable, which makes it suitable for optimization algorithms that rely on gradient-based methods.\n",
    "\n",
    "##### Disadvantages:\n",
    "1. **Sensitivity to Outliers**: The squaring of errors means that MSE can be overly influenced by outliers, which can lead to a misleadingly high error if there are a few large errors.\n",
    "2. **Interpretability**: The unit of MSE is the square of the unit of the dependent variable, which can make it less interpretable compared to metrics that are in the same unit as the dependent variable.\n",
    "\n",
    "#### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "##### Advantages:\n",
    "1. **Same Unit as Dependent Variable**: RMSE is in the same unit as the dependent variable, making it more interpretable and easier to understand in the context of the problem.\n",
    "2. **Sensitivity to Large Errors**: Like MSE, RMSE penalizes larger errors more heavily, which can be useful if large errors are particularly critical.\n",
    "\n",
    "##### Disadvantages:\n",
    "1. **Sensitivity to Outliers**: RMSE, like MSE, can be heavily influenced by outliers due to the squaring of errors.\n",
    "2. **Interpretability**: While more interpretable than MSE, RMSE can still be less intuitive compared to MAE because it involves the square root.\n",
    "\n",
    "#### 3. Mean Absolute Error (MAE)\n",
    "\n",
    "##### Advantages:\n",
    "1. **Robust to Outliers**: MAE is less sensitive to outliers compared to MSE and RMSE because it does not square the errors.\n",
    "2. **Interpretability**: MAE is in the same unit as the dependent variable, making it easy to understand and interpret.\n",
    "\n",
    "##### Disadvantages:\n",
    "1. **Equal Weight to Errors**: MAE treats all errors equally, which may not be desirable in all situations. If large errors are particularly problematic, MAE may not penalize them enough.\n",
    "2. **Non-differentiability at Zero**: The absolute value function is not differentiable at zero, which can be a limitation for some optimization algorithms that require gradient information.\n",
    "\n",
    "### When to Use Each Metric\n",
    "\n",
    "#### RMSE:\n",
    "- **When Large Errors Matter**: RMSE is preferred when you want to penalize large errors more heavily. This can be important in applications where large errors have a disproportionately negative impact.\n",
    "- **Model Interpretation**: RMSE is useful when you want an error metric that is in the same unit as the dependent variable, making it easier to interpret the magnitude of errors.\n",
    "\n",
    "#### MSE:\n",
    "- **Optimization**: MSE is suitable for optimization problems where differentiability is required, as it provides smooth gradients for gradient-based optimization algorithms.\n",
    "- **Penalty for Large Errors**: Like RMSE, MSE is useful when larger errors need to be penalized more significantly, although it can be less interpretable due to the unit issue.\n",
    "\n",
    "#### MAE:\n",
    "- **Robustness to Outliers**: MAE is preferred in situations where outliers are present, and you do not want them to disproportionately influence the error metric.\n",
    "- **Interpretability**: MAE is easy to interpret and provides a straightforward measure of average error in the same unit as the dependent variable.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **RMSE and MSE** are useful when large errors need to be penalized more heavily and when differentiability is important for optimization. However, they can be sensitive to outliers.\n",
    "- **MAE** provides a robust and interpretable measure of error that is less influenced by outliers but does not penalize large errors as heavily and is not differentiable at zero.\n",
    "\n",
    "The choice of metric depends on the specific requirements of the problem, such as the importance of penalizing large errors, the presence of outliers, and the need for interpretability and differentiability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0d2c4-582c-4f05-ae0b-03ed4a568aed",
   "metadata": {},
   "source": [
    "**Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7ed82-2126-4337-b126-0b88446803b1",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "### Lasso Regularization\n",
    "\n",
    "#### Concept\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression to prevent overfitting by adding a penalty to the model's complexity. The penalty term in Lasso regularization is the sum of the absolute values of the coefficients.\n",
    "\n",
    "#### Formula\n",
    "The Lasso regression objective function is:\n",
    "\n",
    "\\[ \\min \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\} \\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the observed value.\n",
    "- \\( x_{ij} \\) is the predictor value.\n",
    "- \\( \\beta_j \\) is the coefficient for the predictor.\n",
    "- \\( \\lambda \\) is the regularization parameter controlling the penalty's strength.\n",
    "- \\( n \\) is the number of observations.\n",
    "- \\( p \\) is the number of predictors.\n",
    "\n",
    "### Differences Between Lasso and Ridge Regularization\n",
    "\n",
    "#### Ridge Regularization\n",
    "Ridge regularization (also known as Tikhonov regularization) adds a penalty equal to the sum of the squared values of the coefficients. The objective function for Ridge regression is:\n",
    "\n",
    "\\[ \\min \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right\\} \\]\n",
    "\n",
    "#### Key Differences\n",
    "\n",
    "1. **Penalty Term**:\n",
    "   - **Lasso**: Uses the sum of the absolute values of the coefficients (\\( \\sum_{j=1}^{p} |\\beta_j| \\)).\n",
    "   - **Ridge**: Uses the sum of the squared values of the coefficients (\\( \\sum_{j=1}^{p} \\beta_j^2 \\)).\n",
    "\n",
    "2. **Effect on Coefficients**:\n",
    "   - **Lasso**: Can shrink some coefficients to exactly zero, effectively performing variable selection. This means it can produce sparse models that are easier to interpret.\n",
    "   - **Ridge**: Shrinks coefficients but does not set them to zero. It tends to distribute the penalty more evenly among all coefficients.\n",
    "\n",
    "3. **Model Interpretation**:\n",
    "   - **Lasso**: Often leads to simpler models by selecting a subset of the original predictors, making it easier to interpret.\n",
    "   - **Ridge**: Produces models where all predictors remain in the model but with reduced coefficients, making it less interpretable when many predictors are involved.\n",
    "\n",
    "### When to Use Lasso Regularization\n",
    "\n",
    "1. **Feature Selection**:\n",
    "   - **Use Lasso** when you suspect that many predictors may be irrelevant or when you want to identify and select a smaller subset of important predictors. Lasso's ability to set coefficients to zero makes it effective for feature selection.\n",
    "\n",
    "2. **Simplicity and Interpretability**:\n",
    "   - **Use Lasso** when you prioritize a simpler, more interpretable model. By eliminating less important features, Lasso can provide a clearer understanding of the relationships between the predictors and the response variable.\n",
    "\n",
    "3. **High-Dimensional Data**:\n",
    "   - **Use Lasso** when dealing with high-dimensional data where the number of predictors exceeds the number of observations. Lasso can help in reducing dimensionality and improving model performance by selecting a smaller subset of relevant predictors.\n",
    "\n",
    "### When to Use Ridge Regularization\n",
    "\n",
    "1. **Collinearity**:\n",
    "   - **Use Ridge** when predictors are highly collinear (i.e., when there is multicollinearity). Ridge regression can handle multicollinearity better by shrinking coefficients and stabilizing the estimates.\n",
    "\n",
    "2. **Small Coefficients**:\n",
    "   - **Use Ridge** when you believe that all predictors are potentially relevant and you want to keep them in the model but reduce their impact. Ridge regression keeps all predictors but reduces their magnitude.\n",
    "\n",
    "3. **Avoiding Overfitting**:\n",
    "   - **Use Ridge** when the primary goal is to improve the predictive performance of the model by preventing overfitting without necessarily performing feature selection.\n",
    "\n",
    "### Example Scenarios\n",
    "\n",
    "- **Lasso**: Suppose you are building a model to predict house prices using a large number of features (e.g., square footage, number of bedrooms, proximity to schools, etc.). If you believe that only a subset of these features is truly important, Lasso can help identify and retain only those relevant features, making the model simpler and more interpretable.\n",
    "\n",
    "- **Ridge**: Suppose you are working on a genetic data analysis where all genes might contribute to the response variable to some extent, but there is high multicollinearity among them. Ridge regression can shrink the coefficients, handle multicollinearity, and improve the model's predictive performance while keeping all genes in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9774b-0208-4a58-b240-7e4ac2b69338",
   "metadata": {},
   "source": [
    "**Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419f3a9-4e64-48b6-971b-825b86dc795c",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty to the loss function, which discourages the model from fitting the noise in the training data. Overfitting occurs when a model captures not only the underlying patterns in the data but also the random noise, leading to poor generalization to new, unseen data.\n",
    "\n",
    "### Regularization Techniques\n",
    "\n",
    "1. **Lasso Regularization (L1 Regularization)**:\n",
    "   - Adds a penalty equal to the sum of the absolute values of the coefficients.\n",
    "   - Objective function: \n",
    "     \\[\n",
    "     \\min \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\}\n",
    "     \\]\n",
    "   - Can set some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "2. **Ridge Regularization (L2 Regularization)**:\n",
    "   - Adds a penalty equal to the sum of the squared values of the coefficients.\n",
    "   - Objective function: \n",
    "     \\[\n",
    "     \\min \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right\\}\n",
    "     \\]\n",
    "   - Shrinks coefficients but does not set them to zero, retaining all features in the model.\n",
    "\n",
    "3. **Elastic Net Regularization**:\n",
    "   - Combines both L1 and L2 penalties.\n",
    "   - Objective function:\n",
    "     \\[\n",
    "     \\min \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j \\right)^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2 \\right\\}\n",
    "     \\]\n",
    "   - Provides a balance between Lasso and Ridge, offering feature selection and stability in the presence of collinearity.\n",
    "\n",
    "### How Regularization Prevents Overfitting\n",
    "\n",
    "1. **Constraining the Model**:\n",
    "   - Regularization constrains the model by adding a penalty to large coefficients. Large coefficients often indicate a model that is too complex and fits the noise in the training data. By shrinking these coefficients, the model becomes simpler and more generalizable.\n",
    "\n",
    "2. **Bias-Variance Trade-off**:\n",
    "   - Regularization introduces bias into the model but reduces variance. This trade-off helps in achieving a model that performs better on new data, as a model with high variance is prone to overfitting.\n",
    "\n",
    "3. **Feature Selection (Lasso)**:\n",
    "   - By setting some coefficients to zero, Lasso regularization effectively reduces the number of features in the model, which can lead to a simpler and more interpretable model that is less likely to overfit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce378139-cfac-4b09-9476-8d3f06f1a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 10\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_coefficients = np.array([5, -3, 0, 0, 2, 0, 0, 1, 0, 0])\n",
    "y = X @ true_coefficients + np.random.randn(n_samples)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5e71b1-9324-4c3e-812b-a7f50f77bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE without regularization: 1.1805\n"
     ]
    }
   ],
   "source": [
    "#Without Regularization\n",
    "\n",
    "# Train linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'MSE without regularization: {mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143eb6a9-95b6-480e-9180-f936f3d19f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE with Ridge regularization: 1.1521\n"
     ]
    }
   ],
   "source": [
    "## With Regularization\n",
    "# Train ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)  # alpha is the regularization strength (lambda)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "ridge_predictions = ridge_model.predict(X_test)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_predictions)\n",
    "print(f'MSE with Ridge regularization: {ridge_mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91c90c4-3271-44aa-ace7-1d8d75b7b782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE with Lasso regularization: 1.0267\n"
     ]
    }
   ],
   "source": [
    "# Train lasso regression model\n",
    "lasso_model = Lasso(alpha=0.1)  # alpha is the regularization strength (lambda)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "lasso_predictions = lasso_model.predict(X_test)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_predictions)\n",
    "print(f'MSE with Lasso regularization: {lasso_mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660afbb-78ee-41df-9f01-7197b3456854",
   "metadata": {},
   "source": [
    "**Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871848c1-4168-43bf-9587-40cfcc475006",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "While regularized linear models, such as Ridge and Lasso regression, have many advantages in preventing overfitting and improving model generalization, they also have certain limitations that might make them less suitable for some regression analysis scenarios. Here are some of the key limitations:\n",
    "\n",
    "### Limitations of Regularized Linear Models\n",
    "\n",
    "1. **Interpretability**:\n",
    "   - **Complexity in Interpretation**: Regularized models can be harder to interpret compared to simple linear regression. While Lasso can make the model more interpretable by selecting features, Ridge retains all features but shrinks their coefficients, which can complicate the interpretation.\n",
    "   - **Non-Zero Coefficients**: In Ridge regression, all coefficients are shrunk but not set to zero, making it difficult to identify the most important features directly.\n",
    "\n",
    "2. **Bias Introduction**:\n",
    "   - **Bias-Variance Trade-off**: Regularization introduces bias into the model to reduce variance. This can sometimes lead to underfitting, especially if the regularization parameter is not tuned properly. Underfitting occurs when the model is too simple to capture the underlying data patterns adequately.\n",
    "\n",
    "3. **Data Scaling Sensitivity**:\n",
    "   - **Need for Standardization**: Regularized models are sensitive to the scale of the input features. It is crucial to standardize or normalize the data before applying regularization. Failing to do so can lead to improper penalization and suboptimal model performance.\n",
    "\n",
    "4. **Choice of Regularization Parameter**:\n",
    "   - **Parameter Tuning**: The effectiveness of regularization depends heavily on the choice of the regularization parameter (lambda). Determining the optimal value of lambda often requires cross-validation, which can be computationally intensive and time-consuming.\n",
    "   - **Over-Penalization**: Incorrectly setting the regularization parameter can lead to over-penalization, where important features are overly shrunk or eliminated, resulting in poor model performance.\n",
    "\n",
    "5. **Handling Multicollinearity**:\n",
    "   - **Limited Multicollinearity Solution**: While Ridge regression can handle multicollinearity to some extent by shrinking correlated features, it does not eliminate it. Lasso can arbitrarily select one feature from a group of correlated features, which may not always be desirable.\n",
    "\n",
    "6. **Non-Linearity**:\n",
    "   - **Linear Assumption**: Regularized linear models assume a linear relationship between the predictors and the response variable. In many real-world scenarios, relationships can be non-linear, and linear models may not capture these complexities well.\n",
    "   - **Model Limitations**: For capturing non-linear relationships, other methods such as polynomial regression, decision trees, or non-linear models (e.g., support vector machines, neural networks) may be more appropriate.\n",
    "\n",
    "7. **Handling High-Dimensional Data**:\n",
    "   - **Scalability Issues**: In cases where the number of features (p) is much larger than the number of observations (n), regularized linear models can struggle to find the optimal solution efficiently. Specialized methods or dimensionality reduction techniques may be needed to handle such high-dimensional data effectively.\n",
    "\n",
    "### Examples When Regularized Linear Models May Not Be the Best Choice\n",
    "\n",
    "1. **Non-Linear Relationships**:\n",
    "   - If the relationship between the predictors and the response variable is non-linear, regularized linear models may perform poorly. For example, predicting stock prices or weather patterns often involves non-linear relationships that linear models cannot capture adequately.\n",
    "\n",
    "2. **Highly Correlated Features**:\n",
    "   - When dealing with highly correlated features, Ridge regression can handle multicollinearity to some extent, but Lasso may arbitrarily select one feature from a correlated group. If the application requires a clear understanding of all correlated features, Ridge may still be limited, and alternative methods like Principal Component Regression (PCR) or Partial Least Squares (PLS) regression may be more appropriate.\n",
    "\n",
    "3. **High-Dimensional, Low-Sample Size Data**:\n",
    "   - In genomic studies or other high-dimensional data scenarios where the number of features far exceeds the number of samples, regularized linear models might struggle. Techniques like Elastic Net, which combines Lasso and Ridge, or advanced methods like random forests, gradient boosting, or deep learning, may provide better results.\n",
    "\n",
    "4. **Feature Interaction**:\n",
    "   - Regularized linear models do not inherently capture interactions between features. If interactions are important, polynomial regression or interaction terms need to be explicitly included in the model, or non-linear methods like decision trees and neural networks might be more suitable.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Regularized linear models like Ridge and Lasso are powerful tools for regression analysis, particularly when dealing with multicollinearity and overfitting. However, their limitations include issues with interpretability, sensitivity to scaling, the need for parameter tuning, and their linear nature, which might not always align with the complexities of real-world data. Understanding these limitations is crucial for selecting the appropriate modeling technique and achieving the best possible outcomes in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679ef69-9a9f-4cd8-a7d2-0f54bf26b93c",
   "metadata": {},
   "source": [
    "**Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b576e-03bb-4690-9c8e-55fba3b35c57",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "When comparing the performance of two regression models using different evaluation metrics, it is crucial to understand what each metric represents and its limitations. \n",
    "\n",
    "### Definitions\n",
    "1. **RMSE (Root Mean Squared Error)**:\n",
    "   - Measures the square root of the average squared differences between predicted and actual values.\n",
    "   - Formula: \n",
    "     \\[\n",
    "     RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "     \\]\n",
    "   - Sensitive to outliers because the differences are squared.\n",
    "\n",
    "2. **MAE (Mean Absolute Error)**:\n",
    "   - Measures the average absolute differences between predicted and actual values.\n",
    "   - Formula:\n",
    "     \\[\n",
    "     MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "     \\]\n",
    "   - Provides a linear score which equally weights all differences.\n",
    "\n",
    "### Comparison of Models\n",
    "\n",
    "- **Model A**: RMSE = 10\n",
    "- **Model B**: MAE = 8\n",
    "\n",
    "### Choosing the Better Performer\n",
    "\n",
    "To determine the better model, consider the following:\n",
    "\n",
    "1. **Understanding the Metrics**:\n",
    "   - **RMSE** penalizes larger errors more than **MAE** because the errors are squared before averaging.\n",
    "   - **MAE** treats all errors equally.\n",
    "\n",
    "2. **Context and Application**:\n",
    "   - **Outliers**: If your data contains significant outliers, RMSE will highlight models that handle large errors poorly, while MAE will give a more balanced view of performance.\n",
    "   - **Domain**: Different domains might prefer one metric over the other. For example, if predicting house prices, large errors might be particularly problematic, making RMSE more relevant.\n",
    "\n",
    "### Comparing RMSE and MAE\n",
    "\n",
    "Since Model A and Model B are evaluated using different metrics, directly comparing them is challenging. However, you can interpret each metric's implications:\n",
    "\n",
    "- **RMSE of 10 for Model A**: Indicates that on average, the squared differences (errors) between predicted and actual values result in a root mean value of 10. It suggests Model A has some larger errors, especially if outliers are present.\n",
    "- **MAE of 8 for Model B**: Indicates that on average, the absolute differences between predicted and actual values are 8. It provides a straightforward average error magnitude.\n",
    "\n",
    "### Limitations of Each Metric\n",
    "\n",
    "1. **RMSE**:\n",
    "   - More sensitive to outliers, which can dominate the metric and give an exaggerated view of model performance if outliers are not representative of the typical data distribution.\n",
    "   - May not be as interpretable in terms of the actual magnitude of typical errors.\n",
    "\n",
    "2. **MAE**:\n",
    "   - Less sensitive to outliers, which can be an advantage or disadvantage depending on the application.\n",
    "   - May under-represent the impact of large errors if they are critical in the specific domain.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "Given only RMSE for Model A and MAE for Model B, it is challenging to make a definitive choice without more context. However, generally:\n",
    "\n",
    "- **If you value** a metric that penalizes large errors more severely (important in many financial or safety-critical applications), RMSE might be more appropriate, making Model A potentially better if you suspect significant outliers.\n",
    "- **If you prefer** a more balanced metric that treats all errors equally and provides a clear interpretation of typical errors, MAE is more suitable, making Model B potentially better for consistent error magnitudes.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "You should ideally evaluate both models using the same metric for a fair comparison. If you had to choose based on the given information:\n",
    "- Consider the nature of your data and the importance of outliers.\n",
    "- If handling large errors is critical, **lean towards RMSE** and Model A.\n",
    "- If consistent performance without over-penalizing outliers is preferred, **lean towards MAE** and Model B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc7803e0-b119-4372-8552-578723bbc3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A - RMSE: 0.5104, MAE: 0.4208\n",
      "Model B - RMSE: 0.5104, MAE: 0.4208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.5, random_state=42)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Model A (Linear Regression)\n",
    "model_A = LinearRegression()\n",
    "model_A.fit(X_train, y_train)\n",
    "\n",
    "# Train Model B (Another Model)\n",
    "model_B = LinearRegression()  # Example, replace with your actual model\n",
    "model_B.fit(X_train, y_train)\n",
    "\n",
    "# Predictions for Model A and Model B on the test set\n",
    "predictions_A = model_A.predict(X_test)\n",
    "predictions_B = model_B.predict(X_test)\n",
    "\n",
    "# Calculate RMSE and MAE for Model A\n",
    "rmse_A = mean_squared_error(y_test, predictions_A, squared=False)\n",
    "mae_A = mean_absolute_error(y_test, predictions_A)\n",
    "\n",
    "# Calculate RMSE and MAE for Model B\n",
    "rmse_B = mean_squared_error(y_test, predictions_B, squared=False)\n",
    "mae_B = mean_absolute_error(y_test, predictions_B)\n",
    "\n",
    "# Print the results\n",
    "print(f'Model A - RMSE: {rmse_A:.4f}, MAE: {mae_A:.4f}')\n",
    "print(f'Model B - RMSE: {rmse_B:.4f}, MAE: {mae_B:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73bddcd-44f3-4551-aa46-a79df974bb20",
   "metadata": {},
   "source": [
    "**Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d350d15-14e5-4a24-b002-556fa186f633",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "To compare the performance of two regularized linear models using different types of regularization (Ridge and Lasso), we need to consider several factors related to each regularization method and their respective parameters.\n",
    "\n",
    "### Model A: Ridge Regularization (Parameter = 0.1)\n",
    "- Ridge regularization adds a penalty to the size of coefficients (L2 norm penalty).\n",
    "- The regularization parameter (alpha = 0.1) controls the strength of regularization:\n",
    "  - Smaller values of alpha imply weaker regularization, allowing coefficients to be closer to those of ordinary least squares (OLS).\n",
    "  - Larger values of alpha increase regularization, shrinking coefficients towards zero more aggressively.\n",
    "\n",
    "### Model B: Lasso Regularization (Parameter = 0.5)\n",
    "- Lasso regularization also penalizes the size of coefficients but uses an L1 norm penalty.\n",
    "- The regularization parameter (alpha = 0.5) similarly controls the strength of regularization:\n",
    "  - Higher values of alpha increase the regularization effect, potentially leading to more coefficients being exactly zero.\n",
    "  - Lower values of alpha reduce the regularization effect, making it more similar to OLS.\n",
    "\n",
    "### Choosing the Better Performer\n",
    "\n",
    "To decide which model performs better, consider the following:\n",
    "\n",
    "1. **Impact on Coefficients**:\n",
    "   - **Ridge**: Typically does not eliminate coefficients completely but shrinks them towards zero.\n",
    "   - **Lasso**: Can lead to sparsity by setting some coefficients exactly to zero, effectively performing feature selection.\n",
    "\n",
    "2. **Application Considerations**:\n",
    "   - **Ridge**: Often preferred when all features are expected to be relevant but some regularization is needed to improve generalization.\n",
    "   - **Lasso**: Preferred when feature selection is desired or when dealing with a high-dimensional dataset with potentially many irrelevant features.\n",
    "\n",
    "3. **Performance Metrics**:\n",
    "   - Evaluate both models using appropriate metrics (such as cross-validated performance, if available) to see which one provides better predictions on unseen data.\n",
    "\n",
    "### Trade-offs and Limitations\n",
    "\n",
    "- **Ridge**: \n",
    "  - **Advantages**: Handles multicollinearity well, stabilizes model performance, and generally prevents overfitting.\n",
    "  - **Limitations**: Does not perform feature selection, so all features remain in the model with non-zero coefficients.\n",
    "\n",
    "- **Lasso**:\n",
    "  - **Advantages**: Can perform feature selection by setting some coefficients to zero, providing a more interpretable model and potentially improving prediction performance.\n",
    "  - **Limitations**: Can be sensitive to correlated predictors (multicollinearity), and the choice of regularization parameter (alpha) is critical. Too high an alpha may lead to underfitting.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Model Choice**: Without specific performance metrics, it's challenging to definitively choose between Ridge and Lasso based solely on the regularization parameters provided (0.1 for Ridge and 0.5 for Lasso).\n",
    "- **General Guidance**: If interpretability and feature selection are crucial, Lasso might be preferred, especially with a higher alpha (0.5). If multicollinearity is a concern and feature selection is less critical, Ridge with a moderate alpha (0.1) could be a good choice.\n",
    "- **Evaluation**: Always validate model performance on unseen data using appropriate metrics to make an informed decision about which model is better suited for your specific dataset and objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89834ae9-968f-4b95-8aa9-4e39fbcb098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Model - RMSE: 0.5193\n",
      "Lasso Model - RMSE: 0.7881\n",
      "\n",
      "Ridge Coefficients: [41.80235952]\n",
      "Lasso Coefficients: [41.25485459]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Define Ridge and Lasso models with specified regularization parameters\n",
    "ridge_model = Ridge(alpha=0.1)\n",
    "lasso_model = Lasso(alpha=0.5)\n",
    "\n",
    "# Train Ridge model\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Train Lasso model\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "ridge_predictions = ridge_model.predict(X_test)\n",
    "lasso_predictions = lasso_model.predict(X_test)\n",
    "\n",
    "# Evaluate RMSE for Ridge model\n",
    "ridge_rmse = mean_squared_error(y_test, ridge_predictions, squared=False)\n",
    "\n",
    "# Evaluate RMSE for Lasso model\n",
    "lasso_rmse = mean_squared_error(y_test, lasso_predictions, squared=False)\n",
    "\n",
    "# Print results\n",
    "print(f'Ridge Model - RMSE: {ridge_rmse:.4f}')\n",
    "print(f'Lasso Model - RMSE: {lasso_rmse:.4f}')\n",
    "\n",
    "# Optionally, you can compare coefficients for Ridge and Lasso models\n",
    "print('\\nRidge Coefficients:', ridge_model.coef_)\n",
    "print('Lasso Coefficients:', lasso_model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7265bd6-775c-4d3c-ab2e-6e4a35e53fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
